---
title: "Getting started with MMIL"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{getting-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  fig.width = 7, 
  warning = FALSE
)
```



```{r setup}
library(mmil)
library(ggplot2)
```


```{r}
#parameters 
rho <- 0.5
zeta <- 0.5
lambda <- 0.1
num_iterations <- 100
```


# Introduction

[start]

# Simulate data

First, we simulate data for this vignette. Specifically, we simulate a dataset of 1000 cells, about half of which are from the disease condition. Each cell is defined by a feature vector of 10 features. To give the disease-associated cells a distinct phenotype from the non-disease-associated cells, the disease-associated cells have half of their features shifted from those of non-disease-associated cells (by adding a constant value of 4 to each disease-associated feature).

The data are simulated such that the disease-associated cells are only present in "patients" who have the disease, but non-disease-associated cells are present in both healthy and sick "patients". Thus, our goal is to find which cells in the patients who have the disease are

```{r}
simulated_data <- 
  mmil_simulate_data(
    num_cells = 1000, 
    num_features = 10, 
    rho = rho, 
    zeta = zeta, 
    shift = 4
  )

# single-cell feature matrix
X <- simulated_data$X

# inherited labels for each cell
z <- simulated_data$z

# true disease-association class for each cell
true_y <- simulated_data$true_y
```

We can visualize the simulated dataset using a scatterplot. Red cells are drawn from sick patients, and blue cells are drawn from healthy patients. Circular cells are from the disease-associated population(s), and triangular cells aren't. 

From this plot, it's easy to see that the disease-associated cells are phenotypically distinct from the non-disease-associated cells - and that their inherited patient-level labels (represented by color) don't correspond to this phenotypic distinction. In short, what we "know" before any modeling are the the inherited patient-level labels (the colors), but what we **want** to know are the true cell-level instance labels (the shapes) because this is where the interesting biology is happening. (For example, we may wish to isolate the disease-associated cells from our patient samples and study them to develop targeted therapies).

```{r} 
# plot data
scatterplot <- 
  prcomp(x = X)$x |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(
    sample_type = dplyr::if_else(z == 0, "healthy sample", "disease sample"), 
    truth = dplyr::if_else(true_y == 0, "healthy cell", "disease cell")
  ) |> 
  ggplot(aes(x = PC1, y = PC2, color = sample_type, shape = truth)) + 
  geom_point(size = 1, alpha = 0.8) + 
  scale_color_brewer(palette = 6, type = "qual") + 
  theme_bw() + 
  labs(
    shape = "True cell-level label", 
    color = "Sample type"
  )

print(scatterplot)
```


# The low-level API 

This package provides several functions for calculating quantities that are needed to apply MMIL to your own model, as long as that model's objective is to optimize the binary classification log-likelihood.  Here are some of those functions: 


## Calculating the case-control intercept 

[A blurb about the case-control intercept.]

```{r}
# Here we address our sampling bias.
# This should be pretty close to 0, because we don't have any sampling bias!
# Including for completeness.
case_control_intercept_adjustment <- 
  mmil_calculate_case_control_adjustment(
    z = z, 
    rho = rho, 
    zeta = zeta
  )

print(round(case_control_intercept_adjustment, 4))
```

## Calculating the log-likelihood

[A blurb about `mmil_loglik`]


```{r}
# Some code

```

## Other low-level functions

* `mmil_initialize_y`
* `mmil_calculate_sample_label_adjustment`
* `mmil_plot_lls`

# The high-level API 

This package also provides a high-level API that can be used to fit `glmnet`, `glm`, or `nnet` models using the Mixture LASSO/MMIL/AlgorithmName schema. 

## Fit model

We can demonstrate some functions in the high-level API using our simulated data. 

### glmnet

```{r}
# fit the model
glmnet_model <- 
  mmil_fit_glmnet(
    X = X, 
    z = z, 
    rho = rho, 
    zeta = zeta, 
    lambda = lambda, 
    case_control_adjustment = case_control_intercept_adjustment, # I am 
    # thinking about removing the case_control_adjustment argument in the 
    # high-level API (i.e. the training functions that we provide) because 
    # the case-control adjustment can be calculated from the other parameters, 
    # so there is some redundancy. We can simply calculate it on the fly instead. 
    # TODO ERIN: What do you think?
    num_iterations = num_iterations, # the max number of iterations for model training
    early_stopping_patience = 3L, # The number of iterations that can have improvements
    # in the LL smaller than `early_stopping_tolerance` before training is stopped
    early_stopping_tolerance = 1e-4 # the smallest improvement for which the 
    # the model will still be considered "improving" by our patience schema
  )

# retrieve LL values for all training iterations
lls <- mmil_get_log_likelihoods(glmnet_model)
```


```{r}
# fit the model
glmnet_model_experimental <- 
  mmil_fit_glmnet(
    X = X, 
    z = z, 
    rho = rho, 
    zeta = zeta, 
    lambda = lambda, 
    case_control_adjustment = case_control_intercept_adjustment, # I am 
    num_iterations = num_iterations, # the max number of iterations for model training
    early_stopping_patience = 3L, # The number of iterations that can have improvements
    # in the LL smaller than `early_stopping_tolerance` before training is stopped
    early_stopping_tolerance = 1e-4 # the smallest improvement for which the 
    # the model will still be considered "improving" by our patience schema
  )

# retrieve LL values for all training iterations
lls_experimental <- mmil_get_log_likelihoods(glmnet_model)
```


```{r}
# plot LL progress
glmnet_model |> 
  mmil_plot_lls() + 
  labs(
    caption = paste0(
      "Notice that the number of max iterations was ", 
      num_iterations, 
      ", but the patience schema stopped training after only ", 
      length(lls), 
      " iterations."
    )
  )

# calculate base (unadjusted predictions) using the trained model
glmnet_predictions_unadjusted <- 
  glmnet_model |> 
  # predictions are probabilities between 0 and 1
  predict(newx = X, s = lambda, type = "response") |> 
  as.numeric()

# calculate predictions with case-control and sample-label adjustments
glmnet_predictions <- 
  glmnet_model |> 
  predict(newx = X, s = lambda, type = "response") |> 
  as.numeric() |> 
  # apply case-control adjustment
  mmil_adjust_predictions_case_control(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # let MMIL know that the predictions are probabilities, not logits (
    # it can accept either when doing the adjustments)
    prediction_type = "prob"
  ) |> 
  # apply sample-label adjustment for cells from sick people
  mmil_adjust_predictions_sample_label(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # optional to set the final model's predictions for cells taken from 
    # healthy patients to a probability of 0 (or logit of -Inf)
    adjust_healthy_samples = FALSE, 
    prediction_type = "prob"
  )

glmnet_performance_tibble <- 
  tibble::tibble(
  glmnet_predictions = glmnet_predictions, 
  truth = dplyr::if_else(true_y == 0, "healthy", "disease"), 
  z = z, 
  sample_type = dplyr::if_else(z == 0, "healthy sample", "disease sample")
)

# plot adjusted probabilities
glmnet_performance_tibble |> 
  ggplot(aes(y = glmnet_predictions, x = truth, fill = sample_type)) + 
  geom_boxplot(position = position_dodge(preserve = "single")) + 
  scale_x_discrete(drop = FALSE) + 
  scale_y_continuous(limits = c(0, 1)) + 
  scale_color_discrete(drop = FALSE) + 
  theme_bw() + 
  labs(subtitle = "glmnet performance", x = "true label", y = "predicted probability")
```

### nnet

What if instead of using a glmnet model, we used a perceptron? 

```{r}
nnet_model <- 
  mmil_fit_mlp(
    X = X, 
    z = z, 
    rho = rho, 
    zeta = zeta, 
    num_iterations = num_iterations, 
    num_neurons = 5, 
    decay = 0.4, 
    maxit = 100
  )

lls <- 
  nnet_model |> 
  mmil_get_log_likelihoods()

```


```{r}
# plot LL at each iteration 
nnet_model |> 
  mmil_plot_lls() + 
  labs(
    caption = paste0(
      "Notice that the number of max iterations was ", 
      num_iterations, 
      ", but the patience schema stopped training after only ", 
      length(lls), 
      " iterations."
    )
  )

nnet_predictions <- 
  nnet_model |> 
  predict(newdata = as.data.frame(X), adjust = FALSE, type = "raw") |> 
  as.numeric() |> 
  # apply case-control adjustment
  mmil_adjust_predictions_case_control(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # let MMIL know that the predictions are probabilities, not logits (
    # it can accept either when doing the adjustments)
    prediction_type = "prob"
  ) |> 
  # apply sample-label adjustment for cells from sick people
  mmil_adjust_predictions_sample_label(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # optional to set the final model's predictions for cells taken from 
    # healthy patients to a probability of 0 (or logit of -Inf)
    adjust_healthy_samples = FALSE, 
    prediction_type = "prob"
  )

nnet_performance_tibble <- 
  tibble::tibble(
    nnet_predictions = nnet_predictions, 
    truth = dplyr::if_else(true_y == 0, "healthy", "disease"), 
    z = z, 
    sample_type = dplyr::if_else(z == 0, "healthy sample", "cancer sample")
  ) 


nnet_performance_tibble |> 
  ggplot(aes(y = nnet_predictions, x = truth, fill = sample_type)) + 
  geom_boxplot(position = position_dodge(preserve = "single")) + 
  scale_x_discrete(drop = FALSE) + 
  scale_color_discrete(drop = FALSE) + 
  scale_y_continuous(limits = c(0, 1)) + 
  theme_bw() + 
  labs(subtitle = "Perceptron performance", x = "true label", y = "predicted probability")
```


### pseudobinomial (pseudo-logistic regression)

What if instead of using either of the two previous models, we used logistic regression (using a pseudobinomial fit to accommodate the soft labels)?

```{r, warning = FALSE}
glm_model <- 
  mmil_fit_glm(
    X = X, 
    z = z, 
    rho = rho, 
    zeta = zeta, 
    num_iterations = num_iterations
  )

lls <- 
  glm_model |> 
  mmil_get_log_likelihoods()
```



```{r}
glm_model |> 
  mmil_plot_lls() + 
  labs(
    caption = paste0(
      "Notice that the number of max iterations was ", 
      num_iterations, 
      ", but the patience schema stopped training after only ", 
      length(lls), 
      " iterations."
    )
  )

glm_predictions <- 
  glm_model |> 
  predict(newdata = as.data.frame(X), type = "response") |> 
  # apply case-control adjustment
  mmil_adjust_predictions_case_control(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # let MMIL know that the predictions are probabilities, not logits (
    # it can accept either when doing the adjustments)
    prediction_type = "prob"
  ) |> 
  # apply sample-label adjustment for cells from sick people
  mmil_adjust_predictions_sample_label(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # optional to set the final model's predictions for cells taken from 
    # healthy patients to a probability of 0 (or logit of -Inf)
    adjust_healthy_samples = FALSE, 
    prediction_type = "prob"
  )

glm_performance_tibble <- 
  tibble::tibble(
    glm_predictions = glm_predictions, 
    truth = dplyr::if_else(true_y == 0, "healthy", "disease"), 
    z = z, 
    sample_type = dplyr::if_else(z == 0, "healthy sample", "cancer sample")
  ) 

glm_performance_tibble |> 
  ggplot(aes(y = glm_predictions, x = truth, fill = sample_type)) + 
  geom_boxplot(position = position_dodge(preserve = "single")) + 
  scale_x_discrete(drop = FALSE) + 
  scale_color_discrete(drop = FALSE) + 
  theme_bw() + 
  labs(subtitle = "Logistic regression performance", x = "true label", y = "predicted probability")
```

## Compare models

We can compare all the models we've fit so far. 

```{r}
model_plot <- 
  glmnet_performance_tibble |> 
  dplyr::mutate(
    glm_predictions = glm_performance_tibble$glm_predictions, 
    nnet_predictions = nnet_performance_tibble$nnet_predictions
  ) |> 
  tidyr::pivot_longer(
    cols = ends_with("_predictions"), 
    names_to = "model", 
    values_to = "prediction"
  ) |> 
  dplyr::mutate(
    model = 
      stringr::str_remove(model, "_predictions") |> 
      factor(levels = c("glmnet", "glm", "nnet"))
  ) |> 
  ggplot(aes(x = sample_type, y = prediction, fill = truth)) +
  geom_boxplot(position = position_dodge(preserve = "single")) + 
  scale_color_discrete(drop = FALSE) + 
  facet_grid(cols = vars(model)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(size = 8, angle = 20, hjust = 1)) + 
  labs(
    y = "MMIL prediction", 
    fill = "True cell-level label", 
    x = "Sample label"
  )

print(model_plot)

```


The coefficients make sense because we have 10 features, the first 5 of which actually differ between the disease-associated and non-disease-associated cells. 

```{r}
coef(glmnet_model, s = mmil_get_hyperparameters(glmnet_model)$lambda) |> 
  as.matrix() |> 
  dplyr::as_tibble(rownames = "feature") |> 
  dplyr::transmute(
    coefficient = s1, 
    feature = forcats::fct_reorder(feature, abs(coefficient))
  ) |> 
  dplyr::filter(abs(coefficient) > 0, feature != "(Intercept)") |> 
  ggplot(aes(x = abs(coefficient), y = feature)) + 
  geom_col() + 
  theme_bw()
```



# Semi-supervised case 

```{r}
# single-cell feature matrix
X |> 
  dim()

# inherited labels for each cell
z

# true disease-association class for each cell
my_true_y <- rep(NA, length(z))
my_true_y[z == 0] <- 0
my_true_y[true_y == 1 & is.na(my_true_y)] <- 
  sample(
    x = c(NA, 1), 
    size = length(my_true_y[true_y == 1 & is.na(my_true_y)]), 
    replace = TRUE, 
    prob = c(0.75, 0.25)
  )
my_true_y

tibble::tibble(
  z = z, 
  true_y = true_y, 
  my_true_y = my_true_y
) |> 
  dplyr::count(z, true_y, my_true_y)
```


```{r}
ss_glmnet_model <- 
  mmil_fit_glmnet(
    X = X, 
    z = z, 
    true_y = my_true_y, 
    rho = rho, 
    zeta = zeta, 
    lambda = lambda, 
    case_control_adjustment = case_control_intercept_adjustment,
    num_iterations = num_iterations, # the max number of iterations for model training
    early_stopping_patience = 3L, # The number of iterations that can have improvements
    # in the LL smaller than `early_stopping_tolerance` before training is stopped
    early_stopping_tolerance = 1e-4 # the smallest improvement for which the 
    # the model will still be considered "improving" by our patience schema
  )

# retrieve LL values for all training iterations
lls <- mmil_get_log_likelihoods(ss_glmnet_model)
```


```{r}
ss_glmnet_model |> 
  mmil_plot_lls()
```


```{r}

# calculate base (unadjusted predictions) using the trained model
ss_glmnet_predictions_unadjusted <- 
  ss_glmnet_model |> 
  # predictions are probabilities between 0 and 1
  predict(newx = X, s = lambda, type = "response") |> 
  as.numeric()

# calculate predictions with case-control and sample-label adjustments
glmnet_predictions <- 
  ss_glmnet_model |> 
  predict(newx = X, s = lambda, type = "response") |> 
  as.numeric() |> 
  # apply case-control adjustment
  mmil_adjust_predictions_case_control(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # let MMIL know that the predictions are probabilities, not logits (
    # it can accept either when doing the adjustments)
    prediction_type = "prob"
  ) |> 
  # apply sample-label adjustment for cells from sick people
  mmil_adjust_predictions_sample_label(
    z = z, 
    rho = rho, 
    zeta = zeta, 
    # optional to set the final model's predictions for cells taken from 
    # healthy patients to a probability of 0 (or logit of -Inf)
    adjust_healthy_samples = FALSE, 
    prediction_type = "prob"
  )

glmnet_performance_tibble <- 
  tibble::tibble(
  glmnet_predictions = glmnet_predictions, 
  truth = dplyr::if_else(true_y == 0, "healthy", "disease"), 
  z = z, 
  sample_type = dplyr::if_else(z == 0, "healthy sample", "disease sample")
)

# plot adjusted probabilities
glmnet_performance_tibble |> 
  ggplot(aes(y = glmnet_predictions, x = truth, fill = sample_type)) + 
  geom_boxplot(position = position_dodge(preserve = "single")) + 
  scale_x_discrete(drop = FALSE) + 
  scale_y_continuous(limits = c(0, 1)) + 
  scale_color_discrete(drop = FALSE) + 
  theme_bw() + 
  labs(subtitle = "glmnet performance", x = "true label", y = "predicted probability")
```


